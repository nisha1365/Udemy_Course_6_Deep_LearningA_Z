{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 14: RNN Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 62: The Vanishing Gradient Problem \n",
    "- The Neural Network travels thru time rather than just from back and front propogations\n",
    "- <img src=\"../images/rnn_01.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- For the rnn,we see that it works a lot like the previous networks. We update the weighted (that come in), find a resoultion, and then based on the cost, we update the cost. However...\n",
    "    - One thing that is different is the use of its contempories network\n",
    "- It should be obvious to you that information from the past is crucial for outputting a sane and plausible prediction. But traditional neural networks can’t do this because they operate on the fundamental assumption that inputs are independent!\n",
    "- This is a problem because it means our output at any given time is completely and solely determined by the input at that same time.\n",
    "- OUTPUT of a layer is added to the next input, and feedback to the some layer\n",
    "- Think: so instead of using a lot of indepdent neurons, we have indpendent neurons but that each affect the next one. Notice in the prev example, every indepedent variables had the affect of only changing the outcome. Now we now that the indepedent variable will travel thru time to affect the next neurons!\n",
    "- The bad thing thing is that you have a lot input, and the gradient descent continue to gets smaller as we go back in time\n",
    "- <img src=\"../images/rnn_02.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "- https://kevinzakka.github.io/2017/07/20/rnn/\n",
    "\n",
    "\n",
    "- From Quora:\n",
    "    - Problem: Gradient based methods learn a parameter's value by understanding how a small change in the parameter's value will affect the network's output. If a change in the parameter's value causes very small change in the network's output - the network just can't learn the parameter effectively, which is a problem.\n",
    "    - This is exactly what's happening in the vanishing gradient problem -- the gradients of the network's output with respect to the parameters in the early layers become extremely small. That's a fancy way of saying that even a large change in the value of parameters for the early layers doesn't have a big effect on the output. Let's try to understand when and why does this problem happen.\n",
    "    \n",
    "- Quick reminder: when we look at the values of the function, we realize that we can have a lot of linear regression or regression that can look for different things. We do not only have to have one kind of value, and each of these regression are only affected by the output. Thus, if we have an output that looks at 2, then we have specific values are that are weighted and activated\n",
    "- The activation is the actual input, or the calcualtion from previous weights and activation and the weight are the function we are trying to perfect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 63: Long Short Term Memory\n",
    "- The reason that the gradient gets smaller is that we conduct the gradients (the gradients of the gradients), the results is that it will get smaller as we move towards the beginning\n",
    "- One way to solve this is to make the Weight Rec = 1\n",
    "- <img src=\"../images/rnn_03.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "- <img src=\"../images/rnn_04.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "- <img src=\"../images/rnn_05.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "- The pipeline at the top  of the image is where we  have the Wrec = 1\n",
    "- There's something called a memory cell which goes thru time (can freely go thru time)\n",
    "- when you backpropogage, this memory cell allievates the issue of the gradient vanishing\n",
    "- <img src=\"../images/rnn_06.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "    - C Stands for memory\n",
    "    - H Stands for the output\n",
    "    - All the values are actual vectors (not a single value)\n",
    "    - Vector Transfer\n",
    "    - Concatenate: you are not really combining, they are really running in parallel feeding into the neural network\n",
    "    - Copy: the vector travels to bit direction\n",
    "    - Pointwise: There are 5 pointwise \n",
    "        - X's: They are valves (from left to right: forget, memory, and output valves)\n",
    "            - Think the valves are real valves where they be either closed or opened (depending on the user)\n",
    "            - If the valve is closed, the memory can flow freely (otherwise, it cannot)\n",
    "            - Each valve is connected to some sigma\n",
    "        - There also something like a t-shaped valve (represented of the + valve)\n",
    "        - <img src=\"../images/rnn_07.png\" alt=\"Drawing\" style=\"width: 200px;\">\n",
    "            - The type of valve is the middle X (where memory is flown from the right to left) and you can add additional memory from the bottom\n",
    "        - Tanh makes the result btw 1 and -1\n",
    "        - <img src=\"../images/rnn_08.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "        - You have vectors controlling the valves\n",
    "- Process\n",
    "    1. <img src=\"../images/rnn_09.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "        - Noticed that you have new information coming in. Moreover, you use the new info with the memory\n",
    "    2. <img src=\"../images/rnn_10.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "        - Yet, its still a bit weird how they are combined within the valves...\n",
    "    3. <img src=\"../images/rnn_11.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "        - The memory flowing thru...\n",
    "    4. <img src=\"../images/rnn_12.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "        - So we do some computation thru the memory flow but all is not used. It then used the info from the current input, and thru some calculations, is decided how exact info will be passed on...\n",
    "- from left to right: forget, memory, and output valves\n",
    "    - The forget memory can tell the network to forget the meaning for a specific observations but to open the memory valve to store the info of the new observation\n",
    "    - from there, we can extract new info on the observations..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 64: Practical Intuition\n",
    "- Most of the time, these networks do not have much going on that we can understand\n",
    "- They are understandable to the machine, but to the human\n",
    "- Similar to how the CNN can understand the small pixel images, these are also networks that can find some method to understand without our help\n",
    "- <img src=\"../images/rnn_13.png\" alt=\"Drawing\" style=\"width: 800px;\">\n",
    "    - The top line represents when the model is not activated (remember: the RNN only activates when it needs to use the memory)\n",
    "    - The 5 lines below the top line are the values that the model is predicting\n",
    "    - Green means active\n",
    "    - Blue means not active\n",
    "    - Red means very likely prediction\n",
    "    - Light red means unlikely prediction\n",
    "    - Noticed that the network only activaes inside URL's\n",
    "    - Using the image, noticed that under the first w, it is predicting that a w will be next (it is correct)\n",
    "    - The second value, it also predicts that it will be a w. After that, it predicts a period (again, correct)\n",
    "    - The confusing part lies in the activation (the network will continue to make prediction) just not with the neuron that activates for a URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 65: LSTM Variation\n",
    "- <img src=\"../images/rnn_14.png\" alt=\"Drawing\" style=\"width: 300px;\">\n",
    "    - Noticed how the image is a bit diff. than your std RNN. You will see what store in the memory and then check if its worth using\n",
    "- <img src=\"../images/rnn_15.png\" alt=\"Drawing\" style=\"width: 300px;\">\n",
    "    - For the image above, we are connecting the forget valve (right) and the memory valve (middle)\n",
    "    - They have a combined decision. When ever you close the forget valve, the memory valve open (same the opposite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Notes\n",
    "- This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data.\n",
    "- <img src=\"../images/rnn_16.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. \n",
    "- Sometimes, we only need to look at recent information to perform the present task\n",
    "    - If we are trying to predict the last word in “the clouds are in the sky,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information.\n",
    "- But there are also cases where we need more context. Consider trying to predict the last word in the text “I grew up in France… I speak fluent French.” \n",
    "    - Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large.\n",
    "- Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.\n",
    "    - <img src=\"../images/rnn_17.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies.\n",
    "    - LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\n",
    "- Regular RNN\n",
    "    - <img src=\"../images/rnn_18.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- RNN (with LSTM)\n",
    "    - <img src=\"../images/rnn_19.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.\n",
    "    - <img src=\"../images/rnn_20.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "- The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!”\n",
    "    - An LSTM has three of these gates, to protect and control the cell state.\n",
    "- The top is simply the info that is being passed, with new ifnomrmation thru the nwtwork,it kind of calculates to see if the info thru the convery belt is actually needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714\n",
    "- https://kevinzakka.github.io/2017/07/20/rnn/\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
