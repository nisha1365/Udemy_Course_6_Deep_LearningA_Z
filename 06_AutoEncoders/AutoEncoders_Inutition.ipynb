{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 25: AutoEncoders Intutition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 135: Plan of Attack\n",
    "- Auto Encoders\n",
    "- Training on an Auto Encoder\n",
    "- Overcomplete Hidden Layers\n",
    "- Sparse Autoencoders\n",
    "- Contractive Autoencoders\n",
    "- Stacked Autoencoders\n",
    "- Deep Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 136: Auto Encoders\n",
    "- You have some outputs and inuts\n",
    "- You can train against the output, to find how well a function is performing\n",
    "- The idea is to have some examples (it could be movies) and then train them\n",
    "- The hidden nodes should be reduced so we have reduce the space (or information requried to decode the visible input nodes)\n",
    "- An autoencoder network is actually a pair of two connected networks, an encoder and a decoder. An encoder network takes in an input, and converts it into a smaller, dense representation, which the decoder network can use to convert it back to the original input.\n",
    "- <img src=\"../images/autoencoder_1.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- Noticed in the image below, we color differentiated the synapses (or the ways that the neurons are connected)\n",
    "- <img src=\"../images/auto_encoder_2.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- Noticed in the image below the hidden nodes are one because they are using syapses and neurons that are connected\n",
    "    - The other nodes do not affect the neurons since they are 0\n",
    "    - <img src=\"../images/auto_encoder_3.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- Noticed to get to the output also requries using the synapses\n",
    "    - Thus, if we have both neurons to 1, and the gray lines (which is -1), would eliminate the blue line (which is +1)\n",
    "    - This explains why some of the output neurons are 0\n",
    "    - <img src=\"../images/auto_encoder_4.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "    - If we apply the softmax function, we assign probabilites. Thus, if the number is 2, we know this will have the highest probability\n",
    "    - <img src=\"../images/auto_encoder_5.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- Realize that the results are very similar to the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lecture 137: A Note on Biases\n",
    "- Remember that there could be biases in the models\n",
    "- The following two images are how biases could be represented\n",
    "- <img src=\"../images/auto_encoder_6.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- <img src=\"../images/auto_encoder_7.png\" alt=\"Drawing\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 138: Training an Auto Encoder\n",
    "- Every single row of your data represented a unique user (someone who rated a movie in our example)\n",
    "- The steps are the following\n",
    "- <img src=\"../images/auto_encoder_8.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- The first step will be to get the data, (the rating will from 1 through 5)\n",
    "- The step 2 is to get the first row (e.g. person)\n",
    "- The step 3 will be calculating the hidden nodes, which will be randomized the first time around\n",
    "- Using those steps, we will then have to calculate the visible output nodes\n",
    "- <img src=\"../images/auto_encoder_9.png\" alt=\"Drawing\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 139: Overcomplete hidden layers\n",
    "- Noticed how in the images above, we only have a few hidden layers\n",
    "- But not use more? It should be obvious that more hidden layers provides more information\n",
    "- The drawback is that if you have the same number of input nodes or more, then the auto encoder can cheat and set one hidden node to every visible node\n",
    "- The next tutorials are three ways to prevent or lessen this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 140: Sparse Autoencoders\n",
    "- The extra number of hidden layers can provides us with features (or feature extraction) which is a positive thing\n",
    "- This a very popular terminonly in deep learning\n",
    "- You are dealing with an autoencoder is when the hidden layer is larger than the visible input nodes\n",
    "- The sparse autoencoder uses a regularization terms\n",
    "- It introduces some sort of barrier\n",
    "- <img src=\"../images/auto_encoder_10.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "    - In the image above, the auto encoder cannot use all the nodes. In the image above, the auto encoder can only use 2 nodes (the whites ones)\n",
    "    - It appears that it randomizes which features it should be use\n",
    "    - The auto encoder does not use alll of the hidden layer at every example (every row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 141: Denoising Autoencoders\n",
    "- <img src=\"../images/auto_encoder_11.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- In the image above, the autoencoder will take some of the input values and make them 0, while keeping the other inputs as they are\n",
    "- You then compare the output with the original set (the set of inputs that are to the left), not the modified inputs\n",
    "- This helps the encoders avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 142: Contractive Autoencoders\n",
    "- The way that it prevents from the output to copy the inputs is to add a penalty in the loss function (when it is backprogagated back)\n",
    "- The exact loss function is not discussed (instructor states that it quite mathematical)\n",
    "- <img src=\"../images/auto_encoder_12.png\" alt=\"Drawing\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 143: Stacked Autoencoders\n",
    "- <img src=\"../images/auto_encoder_13.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- With the stacked autoencoders, the idea is to add more layers to encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 144: Deep Autoencoders\n",
    "- <img src=\"../images/auto_encoder_14.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- These are restriced boltmann machines\n",
    "- RBM that are stacked"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
