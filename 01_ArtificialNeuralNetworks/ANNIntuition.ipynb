{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: ANN Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 7: The Neuron\n",
    "- Axons, Dentrites, and Neurons\n",
    "- Axons are connected to other dentrities!\n",
    "- You generally want the independent variables to be the same.\n",
    "- Output value can be continous (price), binary (yes/no), categorical (discrete).\n",
    "- Think of it like a Multi-Linear Regression where there are many indepedent variables that are for one given possiblity (here, it could categorical).\n",
    "- The synapses here are the weighted (or the line the connect different layers)\n",
    "- The secret is inside the neuron or the brain.\n",
    "- 1st steps: Adds the summation of the weights * input variable\n",
    "- 2nd steps: Performs some kind of calcualtion, a sigmoid function can be a good one because it provides a number between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 8: The Activation Function\n",
    "- We will look at four type of activation functions.\n",
    "    - **Threshold Function:** <img src=\"../images/DeepLearning_3_1.png\" alt=\"Drawing\" style=\"width: 400px;\" >\n",
    "    - If the value is less than 0, then the function passes 0.\n",
    "    - If the value is greater than 0, then the function passes 1.\n",
    "    - **Sigmoid Function:** <img src=\"../images/DeepLearning_3_2.png\" alt=\"Drawing\" style=\"width: 400px;\" >\n",
    "    - Smooth graph, this is good when predicting probability\n",
    "    - **Rectifier:** <img src=\"../images/DeepLearning_3_3.png\" alt=\"Drawing\" style=\"width: 400px;\" >\n",
    "    - Anything below 0, like a negative number, will be 0. From there, it will increase in a linear\n",
    "    - **Hyperbolic Tangent:** <img src=\"../images/DeepLearning_3_4.png\" alt=\"Drawing\" style=\"width: 400px;\" >\n",
    "    - This function is similar to the sigmoid but the values below 0 can go to negative 1.\n",
    "\n",
    "- The hidden layer can have a rectifier function. The signal would be created and passed into the sigmoid function! NEAT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 9: How do Neural Networks work?\n",
    "- At a basic level, say we one the input variable and the output variable, we will only be looking at a multi-variable linear regression!\n",
    "- However, the hidden layer adds the complexity!\n",
    "- All the input variable connect to each of the neuron in the hidden layer!\n",
    "- Not all input will be valid for the neuron.\n",
    "- <img src=\"../images/DeepLearning_3_5.png\" alt=\"Drawing\" style=\"width: 400px;\">\n",
    "- A good example of the Rectifier Function is an example of a house\n",
    "    - Say, if a houses gets to a certain age, say 100, then it will be a historic house which can change the value of the house\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 10: How do Neural Networks learn?\n",
    "- <img src=\"../images/DeepLearning_3_6.png\" alt=\"Drawing\" style=\"width: 800px;\">\n",
    "- You have to code the archiect. There;s no need to code on the specific variables. For example, if you want to distinguish btw cats and dogs, you do not have to explains on the characteristics that make a cat and a dog\n",
    "- **STEPS**\n",
    "    - Weight * Activations + Bias\n",
    "    - Add them all up (in the example above, we would add each of the weight with the activation function).\n",
    "    - Insert the function (sigmoid), which would be a number between 0 and 1.\n",
    "    - Use that as the activations for the weight that are infront of it\n",
    "    - Calculate an estimated answer\n",
    "    - Compare it to the actual!\n",
    "    - Use gradient descent to find the most optimal changes\n",
    "- He argues that the only thing we have control over is the weight, but I think we also have control the betas\n",
    "- We must find the cost function of each of the training example we would use.\n",
    "- From there, we can find the total cost function of each of the training example!\n",
    "- This would be then used to adjust the weight\n",
    "- **THIS IS WHAT WE CALL BACKPROPGATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 11: Gradient Descent\n",
    "- A brute force method would be to try many different methods and see which brings the lowest cost. I have never heard of that idea but it does look like it would work. But the computational power require would be crazy. And it will probably take a lot of time. \n",
    "- <img src=\"../images/DeepLearning_3_7.png\" alt=\"Drawing\" style=\"width: 800px;\">\n",
    "- So apparently, the gradient descent of the cost function does not tell you the magnitude of the direction. Just where you must go!\n",
    "- Remember that the gradient descent is a number that must be multiplied by the learing rate or how fast it converges to the minimum!\n",
    "- It basically calculates the gradient descent, multiplies it with the learing rate and subracts from the original number!\n",
    "- How do we adjust it?\n",
    "    - https://www.youtube.com/watch?v=L-Lsfu4ab74\n",
    "    - Using the gradient descent, the b (or intercept) would change by the error\n",
    "    - Using the gradient descent, the m (or slope) would change by the error, learning rate, and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 12: Stochastic Gradient Descent\n",
    "- It does not require the function to the convex\n",
    "- Take the whole batch of the training examples\n",
    "- Stochastic gradient descent, you change the weights or the parameters after each iterations. With the Batch gradient descent, you change the weights after you run a batch of examples\n",
    "- Batch gradient descent computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution, either local or global. \n",
    "- Stochastic gradient descent (SGD) computes the gradient using a single sample\n",
    "    - It help you advoid the problem of local extremas\n",
    "    - It actually is faster because it does not need to load all the data into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 13: Backpropogation\n",
    "**STEPS**\n",
    "1. Random initiliaze the weight with small numbers (close to 0).\n",
    "2. Input the first observations of your dataset in the input layer, each feature in one input node\n",
    "3. Forward-Propogation: from left to right, the nueron are activated in a way that is impacted by each neuron's activation is limited to the weights. Propogate the activation until the predicted the result of y\n",
    "4. Compare the result to the actual result. Measure the generated error.\n",
    "5. Back-Propogation: from right to left, the error is back-propogated. Update the weights according to how much they are responsible for the error. The learning rate decides by how much we update the weights.\n",
    "6. Repeat steps 1-5 and update the weights after each activation. (Reinforcement Learning or Batch Learning).\n",
    "7. When the whole training set passed through the ANN, that makes an epoch. Redo more epochs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
