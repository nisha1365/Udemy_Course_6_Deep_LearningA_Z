{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 22: Boltzmann Machine Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 106: Plan of attack\n",
    "- What will be learned\n",
    "    - The Botlzmann Machine\n",
    "    - Energy-Based Models (EBM)\n",
    "    - Restricted Botlzmann Machine (RBM)\n",
    "    - Deep Belief Netwrosk (DBN)\n",
    "    - Deep Botlzmann Machines (DBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 107: Boltzmann Machine\n",
    "- <img src=\"../images/boltz_1.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- Noticed how the networks that we have been working follow a direction. Meaning, they follow the path from left to right. \n",
    "- The botlzmann model doesn't not have a specific path, it instead interacts with one another.\n",
    "- <img src=\"../images/boltz_2.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- This model does not have any output layer\n",
    "- Everything is connected with one another\n",
    "- There is no direction\n",
    "- For the botlzmann machine, the nodes are thes same (btw hidden and visible)\n",
    "- For the model, the nodes simply mean data\n",
    "- The botlz machine is a representation of a system. Think of the example he mentioned: the nucleau power plant.\n",
    "    - The nodes represents different \"state\"\n",
    "    - This mean that every node represents a scenario, in the exmaple of the nucleaur power plant, one state could be the pump at a certain level, or a specific pressure within one of the machine \n",
    "- The botlz model tries to learn how the nuclear power plant works (more specificially, our power plants)\n",
    "- Thus, it learns how specific states or nodes work\n",
    "- Then, we can adjust it to model what would happen if the states were changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 108: Energy-Based Models\n",
    "- The botlzmann distribution\n",
    "    - <img src=\"../images/boltz_3.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "    - Looking at the equation above, the higher the e, since the e is a negative exponent, the lower the probability\n",
    "    - The intution is rather simple: IF you are in a room, you are more likely to see the gas among an entire room and not only a specific place. \n",
    "    - If it was specific place, we realize that the prob of that happening is very low. Moreover, the prob. is really low not only because it will be weird to see the gas in one area, but because the energy would be high\n",
    "- The model usues a similar concept, suing the weights, the model will try to find the lowest state that the model can be in\n",
    "- Restriced Boltzmann machine\n",
    "    - <img src=\"../images/boltz_4.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "    - The first to component of the equations are biases\n",
    "    - v is visible node, h is hidden node\n",
    "    - <img src=\"../images/boltz_5.png\" alt=\"Drawing\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 110: Restriced Botlzmann Machines (RBM)\n",
    "- <img src=\"../images/boltz_6.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- Noticed that the machine is different from the the machine we saw above\n",
    "- The machine seems quit complicated\n",
    "- The model works like a recommender system\n",
    "- Thus, we find the rating the users gave movies\n",
    "- From there, we find similar movies. The argument is if the system finds similar movies, they are more likely to be from the same genre (there is no info on the genre of the movie)\n",
    "- The similarity of a movie is constructed on the users and rating, (users will typically like similar movies)\n",
    "- Example: The image below represents an example of someone. Thus, we can see the movies that the have liked, not liked, and have not seen.\n",
    "- <img src=\"../images/boltz_7.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- Like the convolutional network, only certain features are highlighted\n",
    "- Noticed how each of the hidden nodes lights up depdending on the value of the visible nodes\n",
    "- <img src=\"../images/boltz_8.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- We would then figure out how to fix the weight of the nodes\n",
    "- You first run through the model and predict movies you will like using the hidden nodes. \n",
    "- It will then find the movies that are connected with one another and find your specific decision (if you liked the movie or not)\n",
    "- You will use the info that you have established to find if the user would like a movie or not "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 111: Constrastive Divergence\n",
    "- RBM, a bit confused\n",
    "- So far, it is explained the visible nodes create the hidden nodes.\n",
    "    - Then, all of the hidden nodes are used to create the next visible node?\n",
    "- Also, the \"next\" visible node is not the same as the previous node since the hidden nodes are created with all the visible node and not just with one of them\n",
    "- Okay, my way of thinking about it\n",
    "    - If we look at the information from the visible layers, they all combine to make the output layers,\n",
    "    - We should be able to go backwards as well. Meaning, if our weights and biases are accurate, we should be able to use those same weight and biases to get the visible nodes.\n",
    "- Again, this idea is still confusing. But what I am understanding is the following, we are creating these hidden nodes with the visibile nodes.\n",
    "- Using those same hidden nodes,we then recreate to find where it is coming from (the visible nodes)\n",
    "- <img src=\"../images/boltz_9.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- If we look at the exmaple of the moves, we can see that we can predict or check if a movie is genre, use the data input, check what it will gives, and adjust the weight based on the reconstruction of  using the weights and biased for the movie..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 112: Deep Belief Networks\n",
    "- Since RBMs are just a “slice” of a neural network, deep neural networks can be considered to be a bunch of RBMs “stacked” together.\n",
    "- They appear a combination of a lot RMBs\n",
    "- However, in the tutorial, they were not explained as througly, they are complicated models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
