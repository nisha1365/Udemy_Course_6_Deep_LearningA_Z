{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 18: SOMs Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 87: How do SOMs work?\n",
    "- <img src=\"../images/som_01.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "- Noticed how this image represents the poverty of countries\n",
    "- They were able to reduce it to this image using this the SOMs technique\n",
    "- SOMs = Self Organizing Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 88: K-Means Clustering (Refresher)\n",
    "- <img src=\"../images/som_02.png\" alt=\"Drawing\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 90: How do Self-Organizing Map Learn? (Part 1)\n",
    "- THE KEY TO REMEMEBER IS THAT THE WEIGHTS IN THE NODE ARE NOT SUPPOSED TO BE MULTIPLIED WITH THE X'S VECTOR AND THEN ADDED. INSTEAD, THE WEIGHTS ARE A CHARACTERISTICS OF THE NODE. THUS, EACH NODE HAS THREE WEIGHTED WHICH ARE THEN COMPARED TO THE X VECTORS. THUS, FOR ALL THE DATASET, WE ARE COMPUTING THE VALUE OF THE X VECTOR WITH THE NODES! ONCE WE FIND THE DISTANCE AGAINST ALL THE X VECTORS, WE FIND THE CLOSEST NODE THAT STACKS AGAINST THE X VECTORS. THE JOB OF THE NETWORK IT HAVE THE COORINDATES (WEIGHTS) OF THE NODE TO HAVE A BETTER PERFORMANCE... THE THING IS THAT WE WILL HAVE LOT OF BEST MEAN NEURAL NETWORK. WE WILL FOCUS ON MANY POINTS SINCE EACH POINT WILL HAVE A CLOSEST VALUE TO A DIFFERERNT POINT. THE RADIOUS WILL GET CLOSER TO THE WEIGHTED NODE AS WE MOVE THROUGH EPOCHS\n",
    "- <img src=\"../images/som_03.png\" alt=\"Drawing\" style=\"width: 300px;\">\n",
    "- The image above is the same visual represenation as the image below. It is just a different way of presenting the visuals\n",
    "- <img src=\"../images/som_04.png\" alt=\"Drawing\" style=\"width: 300px;\">\n",
    "- But the idea of connecting the networks is very differnt from the neural networks\n",
    "    - Instead, our \"weights,\" are just weight in the same space as our input. Thus, if we have a three input, the first node in output node needs to have these values in their node.\n",
    "    - <img src=\"../images/som_05.png\" alt=\"Drawing\" style=\"width: 300px;\"> \n",
    "    - <img src=\"../images/som_06.png\" alt=\"Drawing\" style=\"width: 300px;\"> \n",
    "- When we find the node values, we are basically applying some kind of function to the node and finding the distance (Euclidean) and checking which node in our output space most likely resembles the input values\n",
    "    - <img src=\"../images/som_07.png\" alt=\"Drawing\" style=\"width: 300px;\"> \n",
    "- Once we have a view on the new weight, we need to focus on how to update the weight and bring it closer to the input\n",
    "- But the input node is not the only weight that gets updated, also the node around the radius\n",
    "    - <img src=\"../images/som_08.png\" alt=\"Drawing\" style=\"width: 300px;\"> \n",
    "- A quick reminder is that every row represents one customer or one value. \n",
    "- Now that way the radius works is that it will change or update the weight accordingly to the way the it got updated based on the specific row. \n",
    "- However, we might have a value to fall within the radious of two values. Thus, we need to change the value by finding the distance and use their distance to how much it will update the new values that fall within the radius\n",
    "    - <img src=\"../images/som_09.png\" alt=\"Drawing\" style=\"width: 300px;\"> \n",
    "- **The weights are the coordinated in the input space**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 91: How do Self-Organizing Maps Learn? (Part 2)\n",
    "- Now, the total number of values is not 2. In fact, looking at the image below, we find out we can have a lot of BMUs (Best Matching Units)\n",
    "    - <img src=\"../images/som_10.png\" alt=\"Drawing\" style=\"width: 300px;\"> \n",
    "- The way these works as that we will continue to reduce the radious after each iterrations\n",
    "    - <img src=\"../images/som_11.png\" alt=\"Drawing\" style=\"width: 300px;\"> \n",
    "- Reminders:\n",
    "    - SOMs retain the topogoly of the input set\n",
    "    - SOMs reveal correlations that are not easily identified\n",
    "    - SOMs classify data without supervision\n",
    "    - No target vector -> no backprogation\n",
    "    - No lateral connection (no connection between the node) btw output nodes\n",
    "- Quick thing to recall is the radius decreased after each epoch\n",
    "    - The mathematics is not explained well. But the author suggest that the math will make the radius closer as we do more iterrations (epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 93: Reading an Advanced SOM\n",
    "- <img src=\"../images/som_12.png\" alt=\"Drawing\" style=\"width: 300px;\"> \n",
    "    - The first graph represents how the SOM divided the clusters. We can see among the differnt questions, these SOMs was able to find how closely the cluster are related\n",
    "        - We can also find the actual groups and how they are divided. Notoced that these groups are not split evenly or how it should be represented based on the political party the congress people claim to be\n",
    "    - Second represenation in the graph (The Unified Distance Matrix)\n",
    "        - Also called the U matrix\n",
    "        - When the points are darker, this mean that the point are further from one another\n",
    "        - When the points are lighter, this means that the points are closer to one another\n",
    "        - If you look at the first diagram, this means that the darker points are closer to ridge. Or they are closer to another. As we move away from the ridge, we realize that these points are not alike hence, the lighter the color\n",
    "    - For the rest:\n",
    "        - Red means republicans voted yes \n",
    "        - Blue means democracts voted yes \n",
    "        - In the first ex: we can see that they bankrupucty chart. If its red, republicans voted yes\n",
    "    - A good way to think abou tis that if we can compare with the actual or the way the cluster predicted, we can see the red for the sub columns to be considered republicans. \n",
    "    - If for some reason, there are a lot of reds, then it must means that even some democrats are considered republican based on that question\n",
    "    - The light yellows shows that there is some inconcistency in the data\n",
    "- One thing to keep in midn is that republicans and demoncracts for the SOm does not mean much. It only means because we can them how related they are to the actual party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 94: EXTRA K-means Clustering (part 2)\n",
    "- <img src=\"../images/som_13.png\" alt=\"Drawing\" style=\"width: 300px;\"> \n",
    "- Noticed that we can find the clusters pretty easily if we begin the clisters at the correct point where we can identify the clusters\n",
    "- But... bad things or a bad result can occur if we choose (randomly) the wrong clusters\n",
    "- <img src=\"../images/som_14.png\" alt=\"Drawing\" style=\"width: 300px;\"> \n",
    "- Noticed how the clusters are differernt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 95: EXTRA K-means Clustering (part 3)\n",
    "- One of the great difficutlies is finding the correct number of clusters to have in our dataset\n",
    "- Another key thing to keep in mind is that we need a method to find if a differernt cluster would have performed better\n",
    "- The setback is that we do not have a metric to use as comparision... until the formula below\n",
    "    - <img src=\"../images/som_15.png\" alt=\"Drawing\" style=\"width: 500px;\"> \n",
    "    - The matheamatics entails that we add every single points distance with the centroid (and square it before you add it)\n",
    "- Example:\n",
    "    - One cluster: (the answer would be large since the points are removed from the centroid)\n",
    "        - <img src=\"../images/som_16.png\" alt=\"Drawing\" style=\"width: 300px;\"> \n",
    "    - Two clusters:\n",
    "        - <img src=\"../images/som_17.png\" alt=\"Drawing\" style=\"width: 300px;\"> \n",
    "    - Three clusters: \n",
    "        - <img src=\"../images/som_18.png\" alt=\"Drawing\" style=\"width: 300px;\"> \n",
    "- Noticed how the WCSS will continue to decrease\n",
    "- If we use each data point as its own cluster, we would have a WCSS of 0. Thus, the WCSS gets better as we make more clusters which is not an ideal feature\n",
    "- In the image below, we can understand that there's diminishing return!\n",
    "- <img src=\"../images/som_19.png\" alt=\"Drawing\" style=\"width: 300px;\"> \n",
    "- <img src=\"../images/som_20.png\" alt=\"Drawing\" style=\"width: 500px;\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://www.ai-junkie.com/ann/som/som2.html\n",
    "- https://deeplearning4j.org/restrictedboltzmannmachine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
